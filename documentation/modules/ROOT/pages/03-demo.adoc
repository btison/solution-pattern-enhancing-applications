= Demo / Walkthrough

Here you find a recorded video of the solution pattern, as well as a walkthrough guide through the different aspects of the solution pattern.

[#demo]
== Recorded demo
video::3yULVMdqJ98[youtube, width=800, height=480]

[#demowalkthrough]
== Demo Walkthrough

This section shows you how you can set up the different parts of the solution pattern, so that you can walk through the demo.

=== Prerequisites:

* Access to an OpenShift environment, preferably with `cluster-admin` privileges.
* A Red Hat account
* `oc` OpenShift CLI
* `helm` Helm CLI

=== 1. API First approach

==== 1.1 API Design with Apicurio Studio

Apicurio Studio  is an environment which allows to collaboratively work on API specifications. It can be installed on OpenShift, but for this demo we use the link:https://studio-auth.apicur.io[free hosted version of Apicurio Studio].

. In a browser window, navigate to link:https://studio-auth.apicur.io[].
. Register as a new user, or log in in using your Github or Google account. +
You are redirected to the dashboard page. From here you can create a new API. You can also import an existing API from e.g. GitHub.
. On the dashboard page, select *Import API* from the *Create new API* drop-down box.
+
image::03/apicurio-import-api.png[]
. On the *Import API* page, set the *Import Type* to *Import From Source Control*. Enter `https://github.com/globex-recommendation/catalog-service-api/blob/main/openapi/openapi-spec.yml` in the *Url* text box. Click *Import API*.
. You are redirected to the `Product Catalog API` front page. This the OpenAPI spec document which describes the API to the *Catalog Service* of the Globex retail application. +
Click *Edit API* to open the API Editor.
+
image::03/apicurio-edit-api.png[]
. Navigate within the editor to inspect the different paths, data types and reusable responses that make up the API definition. You can switching between the graphical editor and the source code editor by selecting the *Design* ot the *Source* tab.

[NOTE]
====
In a real-world scenario you would do the inverse: start with an empty API specification, and define the different elements of the spec document. You then export the spec in JSON or YAML format (by copying the contents from the source editor) to your local file system and push it to version control.
====

==== 1.2 API mocking with Microcks

API mocking allows for parallel streams of development. Developer teams that need to build applications that consume the API don't need to wait until the implementation of the API is finished, but can develop against API mocks that return pre-canned responses.

Microcks is a web-based tool which exposes mocks based on a variety of sources, including OpenAPI spec documents. Refer to the link:https://microcks.io[Microcks web site] for more information.

Microcks needs to be installed on your OpenShift cluster. The easiest way to do this is through the Microcks operator which is available through OperatorHub. This requires cluster admin access. If you don't have cluster admin access, refer to the Microcks installation documentation for alternative installation methods. 

. Clone the https://github.com/globex-recommendation/catalog-service-api repository to your workstation. This repo contains the Catalog Service OpenAPI spec in the `openapi` folder. 
. On the OpenShift cluster, create the `microcks` namespace.
. From the OperatorHub page in the OpenShift web console, install the Microcks operator in the `microcks` namespace (The operator does not support cluster-wide installation mode).
. Deploy a `MicrocksInstall` Custom Resource. The Microcks project provides a minimal Custom Resource which is sufficient for this demo.
+
[.console-input]
[source,bash]
----
OPENSHIFT_DOMAIN=$(oc get ingresscontroller default -n openshift-ingress-operator --template='{{.status.domain}}')
curl https://microcks.io/operator/minikube-minimal.yaml -s | sed "s/KUBE_APPS_URL/${OPENSHIFT_DOMAIN}/g" | oc apply -n microcks -f -
----
. Once the Microcks pods are running, the Microcks console can be reached at https://microcks.$OPENSHIFT_DOMAIN. Log in with `admin/microcks123`
+
image::03/microcks-dashboard.png[]
. Microcks provides several ways to import assets for mocking, including uploading an OpenAPI spec document. +
On the dashboard, click the *Importers* button, and on the *Import Jobs* page, click *Upload*.
. Browse to the `openapi-spec.yml` file in the `openapi` folder of the GitHub project you cloned before and click *Upload* to upload the document.
. On the *APIs|Services* page, click on the *Product Catalog API* link to open the service page.
+
image::03/microcks-service.png[]
. Microcks automatically created a number of mocks based on the examples defined in the OpenAPI spec document for the different paths. Click on the path name to inspect the mocks. Every mock has a unique URL and a pre-determined response.
+
image::03/microcks-mock.png[]

==== 1.3 Publishing and managing APIs with Service Registry

Once a first version of the API specification is ready to be socialized, it can be published in a schema registry, from where other teams can search and find it.

OpenShift Service Registry is a managed cloud service which provides you with an instance of a schema registry, where you can store and manage different kind of schemas, including OpenAPI spec documents and Avro and Protobuf schemas.

[NOTE]
====
The next steps will show you how to upload schema artifacts through the https://console.redhat.com[console.redhat.com] UI. In a more realistic scenario this would be done automatically using the Service Registry REST API as part of a CI/CD pipeline: every time a new version is pushed to source control, a CI/CD pipeline kicks in which publishes the new version in Service Registry.
====

. Navigate to https://console.redhat.com[console.redhat.com] and log in with your Red Hat ID and credentials.
. On the https://console.redhat.com[console.redhat.com] landing page, select *Application Services* from the menu on the left.
. On the Application Services landing page, select *Service Registry → Service Registry instances*.
. On the Service Registry overview page, click the *Create Service Registry instance* button. Enter a unique name like `globex-service-registry` and click *Create* to start the creation process for your Service Registry instance.
. The new Service Registry instance is listed in the instances table. After a couple of seconds, your instance should be marked as ready.
+
image::03/service-registry-instance.png[]
. In the Service Registry instances page of the web console, select the Service Registry instance that you want to upload a schema to.
. Click *Upload artifact* and complete the form to define the schema details. To upload the Product Catalog API specification, browse to the `openapi-spec.yml` file in the `openapi` folder of the GitHub project you cloned before.
+
image::03/service-registry-upload-artifact.png[]
. Once the artifact is uploaded, it can be retrieved from the https://console.redhat.com[console.redhat.com] UI ot through the Service Registry REST API. Validity and compatibility rules can be defined on a per-artifact or global level.
+
image::03/service-registry-artifact.png[]

=== 2. Managed Apache Kafka Cloud Service

OpenShift Streams for Apache Kafka is a managed cloud service that enables you to add Kafka data-streaming functionality in your applications without having to install, configure, run, and maintain your own Kafka clusters.

[NOTE]
====
The following paragraph will guide you through the setup of a managed Kafka instance through the https://console.redhat.com[console.redhat.com] UI.
Red Hat also offers a CLI (`rhoas`) which allows to create and configure a managed Kafka instance though the command line.
====

. Navigate to https://console.redhat.com[console.redhat.com] and log in with your Red Hat ID and credentials.
. On the https://console.redhat.com[console.redhat.com] landing page, select *Application Services* from the menu on the left.
. On the Application Services landing page, select *Streams for Apache Kafka → Kafka Instances*.
. On the Kafka Instances overview page, click the *Create Kafka* instance button. Enter a unique name and select the relevant _Cloud region_ for your Kafka instance and click *Create instance*. This starts the provisioning process for your Kafka instance.
+
[NOTE]
====
Your Red Hat account entitles you to one Kafka instance free of charge. This Kafka instance will stay available for 48 hrs.
====
. When the instance _Status_ is _Ready_, you can start using the Kafka instance.

*Create a Service Account*

To connect applications or services to a Streams for Apache Kafka instance, you need to create a service account.

. On the *Kafka Instances* overview page, select the *Options* icon (the three dots) for the Kafka instance you just created. Select *View connection information*.

. Copy the *Bootstrap server* endpoint to a secure location. You will need this when connecting to your Kafka instance.

. Click *Create service account* to set up the service account. Enter a unique service account name and an optional description, and click *Create*.

. Copy the generated *Client ID* and *Client Secret* to a secure location. These are the credentials that you'll use to connect to this Kafka instance.
+
[IMPORTANT]
====
The generated credentials are displayed only one time, so ensure that you've successfully and securely saved the copied credentials before closing the credentials window. 
====

. After saving the generated credentials, select the confirmation check box and close the Credentials window.

*Set Permissions for a Service Account*

After you creating a service account to connect to a Kafka instance, you must also set the appropriate level of access for that new account in the Access Control List (ACL) of the Kafka instance. Streams for Apache Kafka uses ACLs provided by Kafka that enable you to manage how other user accounts and service accounts are permitted to interact with the Kafka resources that you create.

. On the *Kafka Instances* page, click the name of the Kafka instance you previously created.
. Click the *Access* tab to view the current ACL for this instance.
. Click *Manage access*, use the *Account* drop-down menu to select the service account that you previously created, and click *Next*.
. Under *Assign Permissions*, use the drop-down menus to set the permissions shown in the following table for this service account. Click *Add* to add each new resource permission.
+
These permissions enable applications associated with the service account to create and delete topics in the instance, to produce and consume messages in any topic in the instance, and to use any consumer group and any producer.
+
.ACL permissions for a new service account
[cols="25%,25%,25%,25%"]
|===
h|Resource type
h|Resource identifier and value
h|Access type
h|Operation

|`Topic`
|`Is` = `*`
|`Allow`
|`All`

|`Consumer group`
|`Is` = `*`
|`Allow`
|`Read`

|`Transactional ID`
|`Is` = `*`
|`Allow`
|`All`
|===
+
image::03/rhosak-access-serviceaccount.png[]

*Create a Kafka Topic in OpenShift Streams for Apache Kafka*

The activity tracking functionality of the Globex retail web-site application uses a Kafka topic to store the events generated by the user activity on the web site. This topic needs to be created ahead of time.

. In the *Kafka Instances* page of the web console, click the name of the Kafka instance that you want to add a topic to.

. Select the *Topics* tab, click *Create topic*, and follow the guided steps to define the topic details. Click *Next* to complete each step and click *Finish* to complete the setup.
+
* *Topic name*: `globex-tracking`.
* *Partitions*: `1`
* *Message retention*: Keep the defaults.
